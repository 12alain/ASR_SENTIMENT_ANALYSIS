{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1959962,"sourceType":"datasetVersion","datasetId":1169988}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/working/my_awesome_model'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-29T11:52:50.286357Z","iopub.execute_input":"2024-08-29T11:52:50.287009Z","iopub.status.idle":"2024-08-29T11:52:50.671023Z","shell.execute_reply.started":"2024-08-29T11:52:50.286969Z","shell.execute_reply":"2024-08-29T11:52:50.670067Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/working/my_awesome_model/checkpoint-3000/model.safetensors\n/kaggle/working/my_awesome_model/checkpoint-3000/config.json\n/kaggle/working/my_awesome_model/checkpoint-3000/trainer_state.json\n/kaggle/working/my_awesome_model/checkpoint-3000/rng_state.pth\n/kaggle/working/my_awesome_model/checkpoint-3000/tokenizer.json\n/kaggle/working/my_awesome_model/checkpoint-3000/vocab.txt\n/kaggle/working/my_awesome_model/checkpoint-3000/special_tokens_map.json\n/kaggle/working/my_awesome_model/checkpoint-3000/tokenizer_config.json\n/kaggle/working/my_awesome_model/checkpoint-3000/optimizer.pt\n/kaggle/working/my_awesome_model/checkpoint-3000/scheduler.pt\n/kaggle/working/my_awesome_model/checkpoint-3000/training_args.bin\n/kaggle/working/my_awesome_model/checkpoint-2000/model.safetensors\n/kaggle/working/my_awesome_model/checkpoint-2000/config.json\n/kaggle/working/my_awesome_model/checkpoint-2000/trainer_state.json\n/kaggle/working/my_awesome_model/checkpoint-2000/rng_state.pth\n/kaggle/working/my_awesome_model/checkpoint-2000/tokenizer.json\n/kaggle/working/my_awesome_model/checkpoint-2000/vocab.txt\n/kaggle/working/my_awesome_model/checkpoint-2000/special_tokens_map.json\n/kaggle/working/my_awesome_model/checkpoint-2000/tokenizer_config.json\n/kaggle/working/my_awesome_model/checkpoint-2000/optimizer.pt\n/kaggle/working/my_awesome_model/checkpoint-2000/scheduler.pt\n/kaggle/working/my_awesome_model/checkpoint-2000/training_args.bin\n/kaggle/working/my_awesome_model/runs/Aug29_10-40-42_dff50a835c1b/events.out.tfevents.1724928042.dff50a835c1b.36.2\n/kaggle/working/my_awesome_model/runs/Aug29_10-40-25_dff50a835c1b/events.out.tfevents.1724928026.dff50a835c1b.36.1\n/kaggle/working/my_awesome_model/runs/Aug29_10-06-39_dff50a835c1b/events.out.tfevents.1724926001.dff50a835c1b.36.0\n/kaggle/working/my_awesome_model/checkpoint-1000/model.safetensors\n/kaggle/working/my_awesome_model/checkpoint-1000/config.json\n/kaggle/working/my_awesome_model/checkpoint-1000/trainer_state.json\n/kaggle/working/my_awesome_model/checkpoint-1000/rng_state.pth\n/kaggle/working/my_awesome_model/checkpoint-1000/tokenizer.json\n/kaggle/working/my_awesome_model/checkpoint-1000/vocab.txt\n/kaggle/working/my_awesome_model/checkpoint-1000/special_tokens_map.json\n/kaggle/working/my_awesome_model/checkpoint-1000/tokenizer_config.json\n/kaggle/working/my_awesome_model/checkpoint-1000/optimizer.pt\n/kaggle/working/my_awesome_model/checkpoint-1000/scheduler.pt\n/kaggle/working/my_awesome_model/checkpoint-1000/training_args.bin\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install datasets","metadata":{"execution":{"iopub.status.busy":"2024-08-29T11:52:50.673309Z","iopub.execute_input":"2024-08-29T11:52:50.674450Z","iopub.status.idle":"2024-08-29T11:53:03.738826Z","shell.execute_reply.started":"2024-08-29T11:52:50.674412Z","shell.execute_reply":"2024-08-29T11:53:03.737686Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.21.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.15.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\nRequirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: huggingface-hub>=0.21.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.24.6)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.2)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.7.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import load_dataset\ndata=load_dataset(\"/kaggle/input/allocine-french-movie-reviews\")","metadata":{"execution":{"iopub.status.busy":"2024-08-29T11:53:03.740394Z","iopub.execute_input":"2024-08-29T11:53:03.740767Z","iopub.status.idle":"2024-08-29T11:53:04.776060Z","shell.execute_reply.started":"2024-08-29T11:53:03.740730Z","shell.execute_reply":"2024-08-29T11:53:04.775187Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"data","metadata":{"execution":{"iopub.status.busy":"2024-08-29T17:57:15.009566Z","iopub.execute_input":"2024-08-29T17:57:15.010233Z","iopub.status.idle":"2024-08-29T17:57:15.017891Z","shell.execute_reply.started":"2024-08-29T17:57:15.010192Z","shell.execute_reply":"2024-08-29T17:57:15.016970Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['text', 'label'],\n        num_rows: 160000\n    })\n    validation: Dataset({\n        features: ['text', 'label'],\n        num_rows: 20000\n    })\n    test: Dataset({\n        features: ['text', 'label'],\n        num_rows: 20000\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"# Renommer les colonnes et supprimer les colonnes inutiles\ndata = data.map(lambda x: {\"text\": x[\"review\"], \"label\": x[\"polarity\"]})\ndata = data.remove_columns(['Unnamed: 0', 'film-url', 'review', 'polarity'])\n","metadata":{"execution":{"iopub.status.busy":"2024-08-29T11:53:04.777050Z","iopub.execute_input":"2024-08-29T11:53:04.778065Z","iopub.status.idle":"2024-08-29T11:53:04.806193Z","shell.execute_reply.started":"2024-08-29T11:53:04.778019Z","shell.execute_reply":"2024-08-29T11:53:04.805429Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"#data=load_dataset('imdb')","metadata":{"execution":{"iopub.status.busy":"2024-08-29T11:53:04.813653Z","iopub.execute_input":"2024-08-29T11:53:04.815881Z","iopub.status.idle":"2024-08-29T11:53:04.821127Z","shell.execute_reply.started":"2024-08-29T11:53:04.815839Z","shell.execute_reply":"2024-08-29T11:53:04.820127Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")","metadata":{"execution":{"iopub.status.busy":"2024-08-29T11:53:04.822513Z","iopub.execute_input":"2024-08-29T11:53:04.822783Z","iopub.status.idle":"2024-08-29T11:53:07.127303Z","shell.execute_reply.started":"2024-08-29T11:53:04.822754Z","shell.execute_reply":"2024-08-29T11:53:07.126277Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"def preprocess_function(examples):\n    \n     return tokenizer(examples[\"text\"], truncation=True)\n\n   ","metadata":{"execution":{"iopub.status.busy":"2024-08-29T11:53:07.128340Z","iopub.execute_input":"2024-08-29T11:53:07.128636Z","iopub.status.idle":"2024-08-29T11:53:07.133167Z","shell.execute_reply.started":"2024-08-29T11:53:07.128597Z","shell.execute_reply":"2024-08-29T11:53:07.132304Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"tokenized_imdb = data.map(preprocess_function, batched=True)","metadata":{"execution":{"iopub.status.busy":"2024-08-29T11:53:07.134436Z","iopub.execute_input":"2024-08-29T11:53:07.134723Z","iopub.status.idle":"2024-08-29T11:53:13.136393Z","shell.execute_reply.started":"2024-08-29T11:53:07.134693Z","shell.execute_reply":"2024-08-29T11:53:13.135457Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/20000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"207090d1d03d4d9284bd3a8db9511b9c"}},"metadata":{}}]},{"cell_type":"code","source":"from transformers import DataCollatorWithPadding\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-08-29T11:53:13.137663Z","iopub.execute_input":"2024-08-29T11:53:13.138072Z","iopub.status.idle":"2024-08-29T11:53:16.599787Z","shell.execute_reply.started":"2024-08-29T11:53:13.138027Z","shell.execute_reply":"2024-08-29T11:53:16.598951Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Définir le modèle\nid2label = {0: \"negative\", 1: \"positive\"}\nlabel2id = {\"negative\": 0, \"positive\": 1}","metadata":{"execution":{"iopub.status.busy":"2024-08-29T11:53:16.600867Z","iopub.execute_input":"2024-08-29T11:53:16.601408Z","iopub.status.idle":"2024-08-29T11:53:16.606311Z","shell.execute_reply.started":"2024-08-29T11:53:16.601360Z","shell.execute_reply":"2024-08-29T11:53:16.605414Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"print(tokenized_imdb[\"train\"].features)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-29T11:53:16.607619Z","iopub.execute_input":"2024-08-29T11:53:16.608286Z","iopub.status.idle":"2024-08-29T11:53:16.625004Z","shell.execute_reply.started":"2024-08-29T11:53:16.608240Z","shell.execute_reply":"2024-08-29T11:53:16.624115Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"{'text': Value(dtype='string', id=None), 'label': Value(dtype='int64', id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'token_type_ids': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install transformers datasets evaluate accelerate","metadata":{"execution":{"iopub.status.busy":"2024-08-29T11:53:16.626037Z","iopub.execute_input":"2024-08-29T11:53:16.626311Z","iopub.status.idle":"2024-08-29T11:53:30.019922Z","shell.execute_reply.started":"2024-08-29T11:53:16.626281Z","shell.execute_reply":"2024-08-29T11:53:30.018590Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.44.0)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.21.0)\nRequirement already satisfied: evaluate in /opt/conda/lib/python3.10/site-packages (0.4.2)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.33.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.24.6)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.4)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.7.4)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.13.2)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import evaluate\n\naccuracy = evaluate.load(\"accuracy\")","metadata":{"execution":{"iopub.status.busy":"2024-08-29T11:53:30.021603Z","iopub.execute_input":"2024-08-29T11:53:30.021967Z","iopub.status.idle":"2024-08-29T11:53:31.804178Z","shell.execute_reply.started":"2024-08-29T11:53:30.021930Z","shell.execute_reply":"2024-08-29T11:53:31.803326Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    predictions = np.argmax(predictions, axis=1)\n    return accuracy.compute(predictions=predictions, references=labels)","metadata":{"execution":{"iopub.status.busy":"2024-08-29T11:53:31.807877Z","iopub.execute_input":"2024-08-29T11:53:31.808544Z","iopub.status.idle":"2024-08-29T11:53:31.813990Z","shell.execute_reply.started":"2024-08-29T11:53:31.808504Z","shell.execute_reply":"2024-08-29T11:53:31.812813Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"bert-base-uncased\", num_labels=2, id2label=id2label, label2id=label2id\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-29T11:53:31.815162Z","iopub.execute_input":"2024-08-29T11:53:31.815525Z","iopub.status.idle":"2024-08-29T11:53:32.328322Z","shell.execute_reply.started":"2024-08-29T11:53:31.815489Z","shell.execute_reply":"2024-08-29T11:53:32.327431Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"my_awesome_model\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=2,\n    weight_decay=0.01,\n    eval_strategy=\"steps\",\n    save_strategy=\"steps\",\n    save_steps=1000,  # Sauvegarde toutes les 1000 étapes\n    eval_steps=1000,  # Évaluation toutes les 1000 étapes\n    load_best_model_at_end=True,\n    save_total_limit=3,  # Conserver uniquement les 3 derniers checkpoints\n)\n\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_imdb[\"train\"],\n    eval_dataset=tokenized_imdb[\"validation\"],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)\n\n\ntrainer.train(resume_from_checkpoint=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-29T12:00:21.250396Z","iopub.execute_input":"2024-08-29T12:00:21.251299Z","iopub.status.idle":"2024-08-29T17:08:49.751437Z","shell.execute_reply.started":"2024-08-29T12:00:21.251255Z","shell.execute_reply":"2024-08-29T17:08:49.750536Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3108: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  torch.load(os.path.join(checkpoint, OPTIMIZER_NAME), map_location=map_location)\n/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2843: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint_rng_state = torch.load(rng_file)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='20000' max='20000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [20000/20000 5:08:26, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>4000</td>\n      <td>0.212600</td>\n      <td>0.205355</td>\n      <td>0.933500</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>0.195400</td>\n      <td>0.181482</td>\n      <td>0.939600</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>0.184100</td>\n      <td>0.211350</td>\n      <td>0.936650</td>\n    </tr>\n    <tr>\n      <td>7000</td>\n      <td>0.177900</td>\n      <td>0.168735</td>\n      <td>0.945500</td>\n    </tr>\n    <tr>\n      <td>8000</td>\n      <td>0.174500</td>\n      <td>0.213482</td>\n      <td>0.944050</td>\n    </tr>\n    <tr>\n      <td>9000</td>\n      <td>0.166800</td>\n      <td>0.161974</td>\n      <td>0.947200</td>\n    </tr>\n    <tr>\n      <td>10000</td>\n      <td>0.177200</td>\n      <td>0.161752</td>\n      <td>0.948500</td>\n    </tr>\n    <tr>\n      <td>11000</td>\n      <td>0.134500</td>\n      <td>0.174505</td>\n      <td>0.951350</td>\n    </tr>\n    <tr>\n      <td>12000</td>\n      <td>0.140700</td>\n      <td>0.176011</td>\n      <td>0.948000</td>\n    </tr>\n    <tr>\n      <td>13000</td>\n      <td>0.125400</td>\n      <td>0.176126</td>\n      <td>0.952400</td>\n    </tr>\n    <tr>\n      <td>14000</td>\n      <td>0.138100</td>\n      <td>0.170712</td>\n      <td>0.951550</td>\n    </tr>\n    <tr>\n      <td>15000</td>\n      <td>0.128000</td>\n      <td>0.181787</td>\n      <td>0.951950</td>\n    </tr>\n    <tr>\n      <td>16000</td>\n      <td>0.127800</td>\n      <td>0.171496</td>\n      <td>0.952700</td>\n    </tr>\n    <tr>\n      <td>17000</td>\n      <td>0.122600</td>\n      <td>0.168253</td>\n      <td>0.952650</td>\n    </tr>\n    <tr>\n      <td>18000</td>\n      <td>0.122300</td>\n      <td>0.174152</td>\n      <td>0.952600</td>\n    </tr>\n    <tr>\n      <td>19000</td>\n      <td>0.114400</td>\n      <td>0.167135</td>\n      <td>0.954600</td>\n    </tr>\n    <tr>\n      <td>20000</td>\n      <td>0.126300</td>\n      <td>0.167289</td>\n      <td>0.955150</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=20000, training_loss=0.1291647247314453, metrics={'train_runtime': 18506.9733, 'train_samples_per_second': 17.291, 'train_steps_per_second': 1.081, 'total_flos': 7.550016133545792e+16, 'train_loss': 0.1291647247314453, 'epoch': 2.0})"},"metadata":{}}]},{"cell_type":"code","source":"trainer.save_model(\"my_awesome_model/final_model\")","metadata":{"execution":{"iopub.status.busy":"2024-08-29T17:12:36.281562Z","iopub.execute_input":"2024-08-29T17:12:36.282315Z","iopub.status.idle":"2024-08-29T17:12:37.235275Z","shell.execute_reply.started":"2024-08-29T17:12:36.282269Z","shell.execute_reply":"2024-08-29T17:12:37.234155Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"!zip -r my_awesome_model.zip my_awesome_model/final_model/\n","metadata":{"execution":{"iopub.status.busy":"2024-08-29T17:35:20.772738Z","iopub.execute_input":"2024-08-29T17:35:20.773177Z","iopub.status.idle":"2024-08-29T17:35:44.785440Z","shell.execute_reply.started":"2024-08-29T17:35:20.773137Z","shell.execute_reply":"2024-08-29T17:35:44.784183Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"updating: my_awesome_model/final_model/ (stored 0%)\nupdating: my_awesome_model/final_model/model.safetensors (deflated 7%)\nupdating: my_awesome_model/final_model/config.json (deflated 51%)\nupdating: my_awesome_model/final_model/tokenizer.json (deflated 71%)\nupdating: my_awesome_model/final_model/vocab.txt (deflated 53%)\nupdating: my_awesome_model/final_model/special_tokens_map.json (deflated 42%)\nupdating: my_awesome_model/final_model/tokenizer_config.json (deflated 76%)\nupdating: my_awesome_model/final_model/training_args.bin (deflated 51%)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Préparer les données de test\ntest_dataset = tokenized_imdb[\"test\"]\n\n# Évaluer le modèle sur les données de test\ntest_results = trainer.evaluate(eval_dataset=test_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-08-29T17:58:41.764113Z","iopub.execute_input":"2024-08-29T17:58:41.764536Z","iopub.status.idle":"2024-08-29T18:03:39.502913Z","shell.execute_reply.started":"2024-08-29T17:58:41.764498Z","shell.execute_reply":"2024-08-29T18:03:39.501924Z"},"trusted":true},"execution_count":25,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1250' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1250/1250 04:57]\n    </div>\n    "},"metadata":{}}]},{"cell_type":"code","source":"print(test_results)","metadata":{"execution":{"iopub.status.busy":"2024-08-29T18:04:06.488812Z","iopub.execute_input":"2024-08-29T18:04:06.489605Z","iopub.status.idle":"2024-08-29T18:04:06.496354Z","shell.execute_reply.started":"2024-08-29T18:04:06.489559Z","shell.execute_reply":"2024-08-29T18:04:06.495392Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"{'eval_loss': 0.15533015131950378, 'eval_accuracy': 0.94955, 'eval_runtime': 297.7234, 'eval_samples_per_second': 67.176, 'eval_steps_per_second': 4.199, 'epoch': 2.0}\n","output_type":"stream"}]},{"cell_type":"code","source":"# Enregistrer le modèle évalué\ntrainer.save_model(\"my_awesome_model/final_models\")  # Enregistrement du modèle\ntokenizer.save_pretrained(\"my_awesome_model/final_models\")  # Enregistrement du tokenizer","metadata":{"execution":{"iopub.status.busy":"2024-08-29T18:33:01.605950Z","iopub.execute_input":"2024-08-29T18:33:01.606728Z","iopub.status.idle":"2024-08-29T18:33:02.830674Z","shell.execute_reply.started":"2024-08-29T18:33:01.606687Z","shell.execute_reply":"2024-08-29T18:33:02.829624Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"('my_awesome_model/final_models/tokenizer_config.json',\n 'my_awesome_model/final_models/special_tokens_map.json',\n 'my_awesome_model/final_models/vocab.txt',\n 'my_awesome_model/final_models/added_tokens.json',\n 'my_awesome_model/final_models/tokenizer.json')"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification, AutoTokenizer\n\n# Charger le modèle et le tokenizer depuis le répertoire sauvegardé\nmodel = AutoModelForSequenceClassification.from_pretrained(\"my_awesome_model/checkpoint-20000\")\ntokenizer = AutoTokenizer.from_pretrained(\"my_awesome_model/checkpoint-20000\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-29T19:04:10.753300Z","iopub.execute_input":"2024-08-29T19:04:10.754086Z","iopub.status.idle":"2024-08-29T19:04:10.863208Z","shell.execute_reply.started":"2024-08-29T19:04:10.754044Z","shell.execute_reply":"2024-08-29T19:04:10.862162Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"# Exemple de texte à prédire\ntext = \"le film m'a permet d'avoir des millions de vue sur linedkin\"\n\n# Tokenizer le texte\ninputs = tokenizer(text, return_tensors=\"pt\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-29T19:10:20.848369Z","iopub.execute_input":"2024-08-29T19:10:20.849611Z","iopub.status.idle":"2024-08-29T19:10:20.855962Z","shell.execute_reply.started":"2024-08-29T19:10:20.849559Z","shell.execute_reply":"2024-08-29T19:10:20.854906Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"import torch\n# Faire une prédiction\nwith torch.no_grad():  # Désactiver la calcul de gradients pour l'inférence\n    outputs = model(**inputs)\n\n# Obtenir les logits\nlogits = outputs.logits\n\n# Convertir les logits en prédictions (par exemple, pour une classification)\npredictions = logits.argmax(dim=-1)\nprint(f\"Prédiction : {predictions.item()}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-29T19:10:23.267292Z","iopub.execute_input":"2024-08-29T19:10:23.267754Z","iopub.status.idle":"2024-08-29T19:10:23.352060Z","shell.execute_reply.started":"2024-08-29T19:10:23.267715Z","shell.execute_reply":"2024-08-29T19:10:23.350973Z"},"trusted":true},"execution_count":61,"outputs":[{"name":"stdout","text":"Prédiction : 1\n","output_type":"stream"}]}]}